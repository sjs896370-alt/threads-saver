import streamlit as st
import os

if "browser_fixed" not in st.session_state:
    os.system("playwright install chromium")
    st.session_state.browser_fixed = True

import pandas as pd
from playwright.sync_api import sync_playwright
import time
import json

st.set_page_config(page_title="Threads æ”¶è—å…¨ç´€éŒ„", page_icon="ğŸ§µ")
st.title("ğŸ§µ Threads æ”¶è—å…¨ç´€éŒ„åŒæ­¥")

with st.sidebar:
    st.header("ğŸ”‘ ç™»å…¥è¨­å®š")
    cookie_str = st.text_area("è«‹è²¼å…¥æœ€æ–° JSON Cookies", height=200)
    scroll_times = st.slider("æƒ³è¦å¾€ä¸‹æŒ–å¤šæ·±ï¼Ÿ(æ²å‹•æ¬¡æ•¸)", 1, 30, 10)

if st.button("ğŸš€ é–‹å§‹æ·±åº¦åŒæ­¥æ‰€æœ‰æ”¶è—"):
    if not cookie_str:
        st.error("âŒ è«‹å…ˆè²¼å…¥ Cookiesï¼")
    else:
        with st.spinner("ğŸ•µï¸ æ­£åœ¨æ·±æŒ–æ”¶è—å¤¾ï¼Œé€™å¯èƒ½éœ€è¦ä¸€åˆ†é˜..."):
            try:
                raw_cookies = json.loads(cookie_str)
                cleaned_cookies = []
                for ck in raw_cookies:
                    for d in [".threads.com", ".threads.net"]:
                        new_ck = {
                            "name": ck["name"], "value": ck["value"],
                            "domain": d, "path": "/", "secure": True
                        }
                        ss = str(ck.get("sameSite", "Lax")).capitalize()
                        new_ck["sameSite"] = ss if ss in ["Strict", "Lax", "None"] else "Lax"
                        cleaned_cookies.append(new_ck)

                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    context = browser.new_context(
                        viewport={'width': 1280, 'height': 800},
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
                    )
                    context.add_cookies(cleaned_cookies)
                    page = context.new_page()

                    # å‰å¾€æ”¶è—å¤¾
                    page.goto("https://www.threads.net/settings/saved", wait_until="networkidle")
                    time.sleep(10)

                    all_posts = set() # ä½¿ç”¨é›†åˆä¾†å»é‡
                    
                    # --- æ·±åº¦æ²å‹•é‚è¼¯ ---
                    for i in range(scroll_times):
                        # æŠ“å–ç•¶å‰ç•«é¢æ‰€æœ‰æ–‡å­—å€å¡Š
                        elements = page.locator('span[dir="auto"], div[dir="auto"]').all()
                        for el in elements:
                            txt = el.inner_text().strip()
                            # éæ¿¾é›œè¨Šï¼šé•·åº¦å¤§æ–¼10ï¼Œä¸”ä¸æ˜¯ç³»çµ±æŒ‰éˆ•æ–‡å­—
                            if len(txt) > 10 and not any(x in txt for x in ["Not all", "Log in", "Terms", "Policy", "Back"]):
                                all_posts.add(txt)
                        
                        # åŸ·è¡Œå‘ä¸‹æ»‘å‹•
                        page.mouse.wheel(0, 2000)
                        time.sleep(2) # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                        st.write(f"æ­£åœ¨æƒæç¬¬ {i+1} é å…§å®¹...")

                    if all_posts:
                        data_list = [{"å…§å®¹": p, "åŒæ­¥åºè™Ÿ": i+1} for i, p in enumerate(list(all_posts))]
                        df = pd.DataFrame(data_list)
                        st.success(f"âœ… å¤§åŠŸå‘Šæˆï¼ç¸½å…±æŠ“åˆ° {len(df)} å‰‡è²¼æ–‡ï¼")
                        st.dataframe(df, use_container_width=True)
                        st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´æ”¶è—æ¸…å–®", df.to_csv(index=False, encoding="utf-8-sig").encode('utf-8-sig'), "threads_full_backup.csv")
                    else:
                        st.error("âš ï¸ æƒæå®Œæˆä½†æ²’æŠ“åˆ°å…§å®¹ï¼Œè«‹ç¢ºèªæ‚¨çš„ Cookie æ˜¯å¦å·²éæœŸã€‚")
                    
                    browser.close()
            except Exception as e:
                st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            except Exception as e:
                st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
